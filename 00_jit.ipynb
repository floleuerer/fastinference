{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"name":"00_jit.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"z-oQs1D-uPbg"},"source":["# default_exp jit"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qxPRP8pRuPbi"},"source":["# 00 Jit\n","> Jit support for `fastai` models"]},{"cell_type":"markdown","metadata":{"id":"gGvNII_CuPbj"},"source":["Currently only Vision and Tabular models are supported"]},{"cell_type":"code","metadata":{"id":"qlZn65VkuPbj"},"source":["#hide\n","from nbdev.showdoc import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TI1HJnqbuPbj"},"source":["#export\n","import torch\n","from fastai.learner import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SuD7YsSEuPbj"},"source":["#slow\n","from fastai.vision.all import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dHpFkDusuPbk","outputId":"601df11e-1898-4224-b8ae-0cd4ae154663"},"source":["#slow\n","set_seed(99, True)\n","path = untar_data(URLs.PETS)/'images'\n","dls = ImageDataLoaders.from_name_func(\n","    path, get_image_files(path), valid_pct=0.2,\n","    label_func=lambda x: x[0].isupper(), item_tfms=Resize(224))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"6x_sXuH2uPbk"},"source":["We'll train a quick model to export:"]},{"cell_type":"code","metadata":{"id":"iXVMfou6uPbk","outputId":"a31ff013-5fd0-465a-fcd4-4d39313ba615","colab":{"referenced_widgets":["6e46dd1a55e848bb8428de8e30600781"]}},"source":["#slow\n","learn = cnn_learner(dls, resnet34, metrics=error_rate).to_fp16()\n","learn.fine_tune(1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e46dd1a55e848bb8428de8e30600781","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=87306240.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>error_rate</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.161436</td>\n","      <td>0.026999</td>\n","      <td>0.008119</td>\n","      <td>00:46</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>error_rate</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.059667</td>\n","      <td>0.012131</td>\n","      <td>0.005413</td>\n","      <td>00:49</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"tX0i8qwJuPbl"},"source":["There are two possible scenarios with `jit`: `trace` and `script`. Ideally you should use `torch.jit.script`, however if there is dynamic behavior, `torch.jit.trace` should be utilized instead. As a result `trace` is tried by default\n","\n","`Learner.to_jit()` will perform this decision unless a specific version is specified:\n","\n","Ideally `torch.jit.trace` should be used, as it is built for dynamic behavior (such as CNN's). If your model is not convolutional in nature you should use `trace`"]},{"cell_type":"code","metadata":{"id":"uHO5o-dLuPbl"},"source":["#export\n","mk_class('JitMode', **{o:o.lower() for o in ['Trace','Script']},\n","         doc=\"All possible export modes as attributes to get tab-completion and typo-proofing\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"55tB2F9QuPbl"},"source":["#export\n","@patch\n","def to_jit(self:Learner, fname='export.pt', mode=JitMode.Trace):\n","    \"Exports `learn.model` using `jit` with `mode` to `fname`\"\n","    inp = self.dls.one_batch()[:self.dls.n_inp]\n","    if not isinstance(inp, tuple): inp = (inp,)\n","    self.model.eval()\n","    self.model.to(inp[0].device)\n","    traced_model = getattr(torch.jit, mode)(self.model, inp)\n","    torch.jit.save(traced_model, fname)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HIW0ZmnWuPbl","outputId":"82471c24-95e0-4f57-9c2a-87ff8fdc24ac"},"source":["show_doc(Learner.to_jit)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"Learner.to_jit\" class=\"doc_header\"><code>Learner.to_jit</code><a href=\"__main__.py#L1\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Learner.to_jit</code>(**`fname`**=*`'export.pt'`*, **`mode`**=*`'trace'`*)\n\nExports `learn.model` using `jit` with `mode` to `fname`","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"5YrfKVqEuPbm"},"source":["Below you will find a number of examples using `Learner.to_jit` and loading them back in"]},{"cell_type":"markdown","metadata":{"id":"ECjrlJevuPbm"},"source":["## Tabular (Multi-Input)"]},{"cell_type":"code","metadata":{"id":"nOnF3C9KuPbm"},"source":["from fastai.tabular.all import *\n","path = untar_data(URLs.ADULT_SAMPLE)\n","\n","dls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n","    cat_names = ['workclass', 'education', 'marital-status', 'occupation',\n","                 'relationship', 'race'],\n","    cont_names = ['age', 'fnlwgt', 'education-num'],\n","    procs = [Categorify, FillMissing, Normalize])\n","\n","learn = tabular_learner(dls, metrics=accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2tfB5EotuPbm"},"source":["#hide\n","#slow\n","with tempfile.TemporaryDirectory() as tmpdir:\n","    cat,cont,_ = dls.one_batch()\n","    with torch.no_grad():\n","        learn.model.eval()\n","        learn.model.to(cat.device)\n","        probs = learn.model(cat,cont)\n","    learn.to_jit(f'{tmpdir}/trace.pt', 'trace')\n","    trace = torch.jit.load(f'{tmpdir}/trace.pt', map_location=cat.device)\n","    trace.eval()\n","    probs_jit = trace(cat,cont)\n","    test_close(probs_jit, probs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V3R_W2iLuPbm"},"source":["Tabular models can only be exported with `torch.jit.trace`, so we'll use that:"]},{"cell_type":"code","metadata":{"id":"k6iQEcMGuPbm"},"source":["learn.to_jit('trace.pt', mode=JitMode.Trace)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rG5eTsMcuPbn"},"source":["Now we can load it back in using raw torch and pass in a batch of data:"]},{"cell_type":"code","metadata":{"id":"_7qc7rcpuPbn"},"source":["loaded_model = torch.jit.load(\"trace.pt\")\n","cat,cont,_ = dls.one_batch()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FdVOGdJ6uPbn"},"source":["And perform inference:"]},{"cell_type":"code","metadata":{"id":"96k5CSWYuPbn","outputId":"70cbc714-a722-4995-c286-d391de617d99"},"source":["probs = loaded_model(cat,cont); probs[:3]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0414,  0.0794],\n","        [-0.0426,  0.1249],\n","        [-0.0102,  0.1299]], device='cuda:0', grad_fn=<SliceBackward>)"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"vY7djoHUuPbn"},"source":["> Note: As these are just the models, raw probabilities are returned. You still need to perform a soft or argmax"]},{"cell_type":"markdown","metadata":{"id":"5fmclkweuPbn"},"source":["## Vision"]},{"cell_type":"markdown","metadata":{"id":"vN0vvy-HuPbp"},"source":["Below is an example using `ResNet`:"]},{"cell_type":"code","metadata":{"id":"PwixlzKXuPbp"},"source":["from fastai.vision.all import *\n","path = untar_data(URLs.PETS)/'images'\n","dls = ImageDataLoaders.from_name_func(\n","    path, get_image_files(path), valid_pct=0.2,\n","    label_func=lambda x: x[0].isupper(), item_tfms=Resize(224))\n","learn = cnn_learner(dls, resnet18)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VPgBNZokuPbp"},"source":["#hide\n","#slow\n","with tempfile.TemporaryDirectory() as tmpdir:\n","    x,_ = dls.one_batch()\n","    with torch.no_grad():\n","        learn.model.eval()\n","        learn.model.to(x.device)\n","        probs = learn.model(x)\n","    learn.to_jit(f'{tmpdir}/trace.pt', 'trace')\n","    trace = torch.jit.load(f'{tmpdir}/trace.pt', map_location=x.device)\n","    trace.eval()\n","    probs_trace = trace(x)\n","    test_close(probs, probs_trace)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MDI3Q-0LuPbp"},"source":["Since `ResNet` is a vision model, `trace` should be used:"]},{"cell_type":"code","metadata":{"id":"07-D6IdTuPbp"},"source":["learn.to_jit('trace.pt', mode=JitMode.Trace)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YOWpZjX1uPbp"},"source":["Just as before we can now load it in and perform inference:"]},{"cell_type":"code","metadata":{"id":"XLGIk9kYuPbp","outputId":"dc55eabb-4a48-4b6c-b332-c6ed2622d5f1"},"source":["loaded_model = torch.jit.load(\"trace.pt\")\n","loaded_model.eval()\n","x,_ = dls.one_batch()\n","\n","probs = loaded_model(x); probs[:3]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-1.1999, -2.8738],\n","        [ 5.3266,  1.8526],\n","        [ 0.1073, -0.3077]], device='cuda:0', grad_fn=<SliceBackward>)"]},"metadata":{"tags":[]},"execution_count":0}]}]}