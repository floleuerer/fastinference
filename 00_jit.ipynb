{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"name":"00_jit.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"Dw-UzQ4dvg0o"},"source":["# default_exp jit"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U1_-hnJnvg01"},"source":["# 00 Jit\n","> Jit support for `fastai` models"]},{"cell_type":"markdown","metadata":{"id":"jk8YtWw1vg01"},"source":["Currently only Vision and Tabular models are supported"]},{"cell_type":"code","metadata":{"id":"B6Qm96nCvg01"},"source":["#hide\n","from nbdev.showdoc import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q4eiFP3evg01"},"source":["#export\n","import torch\n","from fastai.learner import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iq3eobhfvg02"},"source":["#slow\n","from fastai.vision.all import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"45tn04xTvg02","outputId":"bafc89b4-94cc-4de4-a156-457d127572fe"},"source":["#slow\n","set_seed(99, True)\n","path = untar_data(URLs.PETS)/'images'\n","dls = ImageDataLoaders.from_name_func(\n","    path, get_image_files(path), valid_pct=0.2,\n","    label_func=lambda x: x[0].isupper(), item_tfms=Resize(224))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"358xdF9Hvg04"},"source":["We'll train a quick model to export:"]},{"cell_type":"code","metadata":{"colab":{"referenced_widgets":["6e46dd1a55e848bb8428de8e30600781"]},"id":"wj8BM0havg04","outputId":"f973e3ba-8095-463c-9263-362713ae20e4"},"source":["#slow\n","learn = cnn_learner(dls, resnet34, metrics=error_rate).to_fp16()\n","learn.fine_tune(1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e46dd1a55e848bb8428de8e30600781","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=87306240.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>error_rate</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.161436</td>\n","      <td>0.026999</td>\n","      <td>0.008119</td>\n","      <td>00:46</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>error_rate</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.059667</td>\n","      <td>0.012131</td>\n","      <td>0.005413</td>\n","      <td>00:49</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"SjZQnKCBvg04"},"source":["There are two possible scenarios with `jit`: `trace` and `script`. Ideally you should use `torch.jit.script`, however if there is dynamic behavior, `torch.jit.trace` should be utilized instead. As a result `trace` is tried by default\n","\n","`Learner.to_jit()` will perform this decision unless a specific version is specified:\n","\n","Ideally `torch.jit.trace` should be used, as it is built for dynamic behavior (such as CNN's). If your model is not convolutional in nature you should use `trace`"]},{"cell_type":"code","metadata":{"id":"ZZgwhGlpvg05"},"source":["#export\n","mk_class('JitMode', **{o:o.lower() for o in ['Trace','Script']},\n","         doc=\"All possible export modes as attributes to get tab-completion and typo-proofing\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9OdnQ1MGvg05"},"source":["#export\n","@patch\n","def to_jit(self:Learner, fname='export.pt', mode=JitMode.Trace):\n","    \"Exports `learn.model` using `jit` with `mode` to `fname`\"\n","    inp = self.dls.one_batch()[:self.dls.n_inp]\n","    if not isinstance(inp, tuple): inp = (inp,)\n","    self.model.eval()\n","    self.model.to(inp[0].device)\n","    traced_model = getattr(torch.jit, mode)(self.model, inp)\n","    torch.jit.save(traced_model, fname)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K9sFeVHOvg05","outputId":"361a5e68-3037-44ef-df8e-2b0630fbb532"},"source":["show_doc(Learner.to_jit)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"Learner.to_jit\" class=\"doc_header\"><code>Learner.to_jit</code><a href=\"__main__.py#L1\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Learner.to_jit</code>(**`fname`**=*`'export.pt'`*, **`mode`**=*`'trace'`*)\n\nExports `learn.model` using `jit` with `mode` to `fname`","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"3uVDOorivg06"},"source":["Below you will find a number of examples using `Learner.to_jit` and loading them back in"]},{"cell_type":"markdown","metadata":{"id":"sOituXs-vg06"},"source":["## Tabular (Multi-Input)"]},{"cell_type":"code","metadata":{"id":"VyH0Yb_Avg06"},"source":["from fastai.tabular.all import *\n","path = untar_data(URLs.ADULT_SAMPLE)\n","\n","dls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n","    cat_names = ['workclass', 'education', 'marital-status', 'occupation',\n","                 'relationship', 'race'],\n","    cont_names = ['age', 'fnlwgt', 'education-num'],\n","    procs = [Categorify, FillMissing, Normalize])\n","\n","learn = tabular_learner(dls, metrics=accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jRTqlKz-vg06"},"source":["#hide\n","#slow\n","with tempfile.TemporaryDirectory() as tmpdir:\n","    cat,cont,_ = dls.one_batch()\n","    with torch.no_grad():\n","        learn.model.eval()\n","        learn.model.to(cat.device)\n","        probs = learn.model(cat,cont)\n","    learn.to_jit(f'{tmpdir}/trace.pt', 'trace')\n","    trace = torch.jit.load(f'{tmpdir}/trace.pt', map_location=cat.device)\n","    trace.eval()\n","    probs_jit = trace(cat,cont)\n","    test_close(probs_jit, probs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R-pSNZl2vg06"},"source":["Tabular models can only be exported with `torch.jit.trace`, so we'll use that:"]},{"cell_type":"code","metadata":{"id":"JnzAy3ujvg06"},"source":["learn.to_jit('trace.pt', mode=JitMode.Trace)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O1FelXY7vg07"},"source":["Now we can load it back in using raw torch and pass in a batch of data:"]},{"cell_type":"code","metadata":{"id":"mgL0EdmIvg07"},"source":["loaded_model = torch.jit.load(\"trace.pt\")\n","cat,cont,_ = dls.one_batch()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NYCcMDecvg07"},"source":["And perform inference:"]},{"cell_type":"code","metadata":{"id":"lwdU4DUPvg07","outputId":"ca8dcbc8-b4aa-4f89-f7f6-86530ef86f02"},"source":["probs = loaded_model(cat,cont); probs[:3]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0414,  0.0794],\n","        [-0.0426,  0.1249],\n","        [-0.0102,  0.1299]], device='cuda:0', grad_fn=<SliceBackward>)"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"Zg7Cxpe7vg07"},"source":["> Note: As these are just the models, raw probabilities are returned. You still need to perform a soft or argmax"]},{"cell_type":"markdown","metadata":{"id":"DhQmNahSvg07"},"source":["## Vision"]},{"cell_type":"markdown","metadata":{"id":"mp0gRoH3vg08"},"source":["Below is an example using `ResNet`:"]},{"cell_type":"code","metadata":{"id":"65d-1yjGvg08"},"source":["from fastai.vision.all import *\n","path = untar_data(URLs.PETS)/'images'\n","dls = ImageDataLoaders.from_name_func(\n","    path, get_image_files(path), valid_pct=0.2,\n","    label_func=lambda x: x[0].isupper(), item_tfms=Resize(224))\n","learn = cnn_learner(dls, resnet18)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xtsc9bLUvg08"},"source":["#hide\n","#slow\n","with tempfile.TemporaryDirectory() as tmpdir:\n","    x,_ = dls.one_batch()\n","    with torch.no_grad():\n","        learn.model.eval()\n","        learn.model.to(x.device)\n","        probs = learn.model(x)\n","    learn.to_jit(f'{tmpdir}/trace.pt', 'trace')\n","    trace = torch.jit.load(f'{tmpdir}/trace.pt', map_location=x.device)\n","    trace.eval()\n","    probs_trace = trace(x)\n","    test_close(probs, probs_trace)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vjsuopbCvg08"},"source":["Since `ResNet` is a vision model, `trace` should be used:"]},{"cell_type":"code","metadata":{"id":"IcokMcmJvg08"},"source":["learn.to_jit('trace.pt', mode=JitMode.Trace)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vf-jaWd5vg08"},"source":["Just as before we can now load it in and perform inference:"]},{"cell_type":"code","metadata":{"id":"ghkVZ0z_vg08","outputId":"7da7c471-d87c-4ce0-8fbd-f932bb386446"},"source":["loaded_model = torch.jit.load(\"trace.pt\")\n","loaded_model.eval()\n","x,_ = dls.one_batch()\n","\n","probs = loaded_model(x); probs[:3]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-1.1999, -2.8738],\n","        [ 5.3266,  1.8526],\n","        [ 0.1073, -0.3077]], device='cuda:0', grad_fn=<SliceBackward>)"]},"metadata":{"tags":[]},"execution_count":0}]}]}